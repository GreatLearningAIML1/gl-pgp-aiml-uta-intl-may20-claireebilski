{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Plant Seedlings Classification Project 7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cVKbxufGwXk",
        "outputId": "f8f11294-551c-4240-fb62-e366b77cf0f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLoQi4aGHb7E"
      },
      "source": [
        "Context: Can you differentiate a weed from a crop seedling?\n",
        "\n",
        "Objective: To implement the techniques learnt as a part of the course.\n",
        "\n",
        "\n",
        " use  the  dataset  path  in  the  Google  Colab  notebook  to  do  further  steps  related  to  project  problem statement.\n",
        "You can set runtime type to “GPU” in Google Colab\n",
        "\n",
        "Steps and tasks:\n",
        "1.Import the libraries, load dataset, print shape of data, visualize the images in dataset. (5 Marks)\n",
        "2.Data Pre-processing: (15 Marks)a.Normalization. b.Gaussian Blurring. c.Visualize data after pre-processing.\n",
        "3.Make data compatible: (10 Marks) a.Convert labels to one-hot-vectors. b.Print the label for y_train[0] .c.Split the dataset into training,testing, and validation set.(Hint: First split images andlabels into training and testing set with test_size = 0.3. Then further split test data into test and validation set with test_size = 0.5) d.Check  the  shape  of  data, Reshape  data  into  shapes compatible with Keras models if it’s not already. If it’s already in the compatible shape, then comment in the notebook that it’s already in compatible shape.\n",
        "4.Building CNN: (15 Marks) a.Define layers. b.Set optimizer and loss function. (Use Adam optimizer and categorical crossentropy.)\n",
        "5.Fit and evaluate model and print confusion matrix. (10 Marks)\n",
        "6.Visualize predictions for x_test[2], x_test[3], x_test[33], x_test[36], x_test[59]. (5 Marks)\n",
        "Note: The data provided to you on Olympus has only train images and their labels. For our purpose we use this for our training and testing and validation purpose.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zF3KjXj0kvrK"
      },
      "source": [
        "# Step 1\n",
        "# Import necessary modules.\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras import datasets, models, layers, optimizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di_a2Ji3SC3u"
      },
      "source": [
        "data = pd.read_csv('/Labels.csv')    # Load the dataset by providing the path to the file."
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBG_uOY2SGo4",
        "outputId": "966a7a7e-5b36-4bbf-bedc-de869e64ebb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Small-flowered Cranesbill</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Small-flowered Cranesbill</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Small-flowered Cranesbill</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Small-flowered Cranesbill</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Small-flowered Cranesbill</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Label\n",
              "0  Small-flowered Cranesbill\n",
              "1  Small-flowered Cranesbill\n",
              "2  Small-flowered Cranesbill\n",
              "3  Small-flowered Cranesbill\n",
              "4  Small-flowered Cranesbill"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6u_09biUSHuq",
        "outputId": "0f9f3f80-d940-4920-e42a-d37181d8c4d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Print the shape of data.\n",
        "data.shape\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4750, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kB-0TB2KzU1h"
      },
      "source": [
        "y = data[\"Label\"]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Npdt8TOzgFX",
        "outputId": "9b088c3a-eade-4066-a19a-a0337479d747",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        }
      },
      "source": [
        "#Visualie the images in the dataset.\n",
        "g = sns.countplot(y)\n",
        "y.value_counts()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Loose Silky-bent             654\n",
              "Common Chickweed             611\n",
              "Scentless Mayweed            516\n",
              "Small-flowered Cranesbill    496\n",
              "Fat Hen                      475\n",
              "Charlock                     390\n",
              "Sugar beet                   385\n",
              "Cleavers                     287\n",
              "Black-grass                  263\n",
              "Shepherds Purse              231\n",
              "Common wheat                 221\n",
              "Maize                        221\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAEGCAYAAADfZmpgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dfbwdVX3v8c+XhGeBADkGCIEgRCgtEDBXQaxFqQooAoqoFUFEY5UqXKst1ntb6JN6vT6AKLdURKC0glgEkaoYQBF5MIGQkARMVB4SCAnIQyAPkOR3//j9ds/meBII7n325OT7fr3O68xeM3vNmjVr1m9mzZw5igjMzMyaaKNeF8DMzGxNHKTMzKyxHKTMzKyxHKTMzKyxHKTMzKyxRva6AENh9OjRMX78+F4Xw8xsvTJt2rRHIqKvl2XYIILU+PHjmTp1aq+LYWa2XpF0X6/L4OE+MzNrLAcpMzNrLAcpMzNrLAcpMzNrLAcpMzNrLAcpMzNrLAcpMzNrLAcpMzNrLAcpMzNrrA3ijRNmZsPNw2fd3JV8x5x6UFfyfbF8JWVmZo3lIGVmZo3lIGVmZo3lIGVmZo3lIGVmZo3lIGVmZo3lIGVmZo3lIGVmZo3lIGVmZo3lIGVmZo3lIGVmZo3lIGVmZo3lF8zaeuOkKw7rWt4XHPODruVtZi9eI66kJI2SdLmkuyXNkXSQpO0kXStpbv3etpaVpLMlzZM0Q9IBvS6/mZl1RyOCFHAW8IOI2AvYD5gDnA5MiYgJwJT6DHA4MKF+JgPnDn1xzcxsKPQ8SEnaBngtcD5ARDwTEY8DRwEX1mIXAkfX9FHARZFuAUZJ2nGIi21mZkOg50EK2A1YDFwg6Q5JX5e0JTAmIh6qZRYCY2p6LPBA2/fnV9pzSJosaaqkqYsXL+5i8c3MrFuaEKRGAgcA50bE/sDT9A/tARARAcS6ZBoR50XEpIiY1NfX17HCmpnZ0GlCkJoPzI+IW+vz5WTQerg1jFe/F9X8BcC4tu/vXGlmZjbM9DxIRcRC4AFJe1bSocBs4CrgxEo7Ebiypq8CTqin/A4EnmgbFjQzs2GkKX8n9VHgEkmbAL8GTiID6GWSTgbuA46rZa8BjgDmAUtrWTMzG4YaEaQiYjowaZBZhw6ybACndL1QZmbWcz0f7jMzM1sTBykzM2ssBykzM2ssBykzM2ssBykzM2ssBykzM2ssBykzM2ssBykzM2ssBykzM2ssBykzM2ssBykzM2ssBykzM2ssBykzM2ssBykzM2ssBykzM2ssBykzM2ssBykzM2ssBykzM2ssBykzM2ssBykzM2uskb0ugJn1xtGXT+lKvt899tCu5GsbJl9JmZlZYzUiSEm6V9JMSdMlTa207SRdK2lu/d620iXpbEnzJM2QdEBvS29mZt3SiCBVXhcREyNiUn0+HZgSEROAKfUZ4HBgQv1MBs4d8pKamdmQaFKQGugo4MKavhA4ui39oki3AKMk7diLApqZWXc1JUgF8CNJ0yRNrrQxEfFQTS8ExtT0WOCBtu/Or7TnkDRZ0lRJUxcvXtytcpuZWRc15em+10TEAkkvBa6VdHf7zIgISbEuGUbEecB5AJMmTVqn75qZWTM0IkhFxIL6vUjSFcArgYcl7RgRD9Vw3qJafAEwru3rO1faC7L43H/rUKmfq+/Dx3clXzOzDVnPh/skbSlpq9Y08EbgLuAq4MRa7ETgypq+CjihnvI7EHiibVjQzMyGkSZcSY0BrpAEWZ5/j4gfSPoFcJmkk4H7gONq+WuAI4B5wFLgpKEvspmZDYWeB6mI+DWw3yDpjwK/86frERHAKUNQNDMz67GeD/eZmZmtiYOUmZk1Vs+H+4a7+88+tiv57vKxy7uSr5lZk/hKyszMGstByszMGstByszMGstByszMGstByszMGstByszMGstByszMGstByszMGstByszMGstByszMGstByszMGstByszMGstByszMGstByszMGstByszMGstByszMGstByszMGstByszMGstByszMGstByszMGqsxQUrSCEl3SLq6Pu8m6VZJ8yRdKmmTSt+0Ps+r+eN7WW4zM+uexgQp4FRgTtvnzwFfiog9gMeAkyv9ZOCxSv9SLWdmZsNQI4KUpJ2BNwNfr88CXg9cXotcCBxd00fVZ2r+obW8mZkNM40IUsCXgb8CVtfn7YHHI2JlfZ4PjK3pscADADX/iVr+OSRNljRV0tTFixd3s+xmZtYlPQ9Skt4CLIqIaZ3MNyLOi4hJETGpr6+vk1mbmdkQGdnrAgAHA2+VdASwGbA1cBYwStLIulraGVhQyy8AxgHzJY0EtgEeHfpim5lZt/U8SEXEp4BPAUg6BPhERLxH0reBY4FvAScCV9ZXrqrPN9f86yIihrrcNvy9+YrPdy3v7x/zya7lbTac9Hy4by3+Gvi4pHnkPafzK/18YPtK/zhweo/KZ2ZmXdbzK6l2EXEDcENN/xp45SDLLAfeMaQFM7Pf28eueKAr+Z59zLiu5GvN0OQrKTMz28B1NEhJmvJC0szMzF6Ijgz3SdoM2AIYLWlboPXHtVvT//dNZmZm66RT96Q+BJwG7ARMoz9IPQmc06F1mJnZBqYjQSoizgLOkvTRiPhKJ/I0MzPr6NN9EfEVSa8GxrfnHREXdXI9Zma2YehokJJ0MbA7MB1YVckBOEiZmdk66/TfSU0C9vYbIMzMrBM6/XdSdwE7dDhPMzPbQHX6Smo0MFvSbcCKVmJEvLXD6zEzsw1Ap4PUGR3Oz8zMNmCdfrrvJ53Mz8zMNmydfrpvCfk0H8AmwMbA0xGxdSfXY2ZmG4ZOX0lt1ZqWJOAo4MBOrsPMzDYcXXsLeqTvAm/q1jrMzGx46/Rw39vaPm5E/t3U8k6uw8zMNhydfrrvyLbplcC95JCfmZnZOuv0PamTOpmfmZlt2Dr9Tw93lnSFpEX18x1JO3dyHWZmtuHo9IMTFwBXkf9Xaifge5VmZma2zjodpPoi4oKIWFk/3wT6OrwOMzPbQHQ6SD0q6XhJI+rneODRDq/DzMw2EJ0OUu8HjgMWAg8BxwLv6/A6zMxsA9HpIPX3wIkR0RcRLyWD1plr+4KkzSTdJulOSbMknVnpu0m6VdI8SZdK2qTSN63P82r++A5vg5mZNUSng9S+EfFY60NE/BbY/3m+swJ4fUTsB0wEDpN0IPA54EsRsQfwGHByLX8y8Filf6mWMzOzYajTQWojSdu2Pkjajuf5W6x6fdJT9XHj+gng9cDllX4hcHRNH1WfqfmH1nsCzcxsmOn0Gye+ANws6dv1+R3APz3flySNAKYBewBfBX4FPB4RK2uR+cDYmh4LPAAQESslPQFsDzwyIM/JwGSAXXbZ5ffYJDMz65WOXklFxEXA24CH6+dtEXHxC/jeqoiYCOwMvBLYqwNlOS8iJkXEpL4+PwVvZrY+6vSVFBExG5j9Ir/7uKTrgYOAUZJG1tXUzsCCWmwBMA6YL2kksA1+zN3MbFjq2r/qeKEk9UkaVdObA28A5gDXk4+wA5wIXFnTV9Vnav51ERGYmdmw0/ErqRdhR+DCui+1EXBZRFwtaTbwLUn/CNwBnF/Lnw9cLGke8FvgXb0otFmnveXyS7qS79XHvqcr+ZoNhZ4HqYiYwSCPqUfEr8n7UwPTl5MPZJiZ2TDX8+E+MzOzNXGQMjOzxnKQMjOzxnKQMjOzxnKQMjOzxnKQMjOzxur5I+hmZsPBvV9e2JV8x5+2Q1fyXV/4SsrMzBrLQcrMzBrLQcrMzBrLQcrMzBrLQcrMzBrLQcrMzBrLQcrMzBrLQcrMzBrLQcrMzBrLb5wYZn54/hFdyfdNJ1/TlXzNzNbGV1JmZtZYDlJmZtZYHu6zF+1fLn5T1/L+0Ht/2LW8bcPwX5c+0pV8D3/n6K7ka4PzlZSZmTWWg5SZmTVWz4OUpHGSrpc0W9IsSadW+naSrpU0t35vW+mSdLakeZJmSDqgt1tgZmbd0vMgBawE/jIi9gYOBE6RtDdwOjAlIiYAU+ozwOHAhPqZDJw79EU2M7Oh0PMgFREPRcTtNb0EmAOMBY4CLqzFLgSOrumjgIsi3QKMkrTjEBfbzMyGQM+DVDtJ44H9gVuBMRHxUM1aCIyp6bHAA21fm19pA/OaLGmqpKmLFy/uWpnNzKx7GhOkJL0E+A5wWkQ82T4vIgKIdckvIs6LiEkRMamvr6+DJTUzs6HSiCAlaWMyQF0SEf9ZyQ+3hvHq96JKXwCMa/v6zpVmZmbDTM+DlCQB5wNzIuKLbbOuAk6s6ROBK9vST6in/A4EnmgbFjQzs2GkCW+cOBh4LzBT0vRK+xvgs8Blkk4G7gOOq3nXAEcA84ClwElDW1wzMxsqPQ9SEfEzQGuYfeggywdwSlcLZWZmjdDz4T4zM7M1cZAyM7PGcpAyM7PGcpAyM7PGcpAyM7PGcpAyM7PGcpAyM7PGcpAyM7PGcpAyM7PGcpAyM7PGcpAyM7PGcpAyM7PGcpAyM7PGcpAyM7PGcpAyM7PGcpAyM7PGcpAyM7PGcpAyM7PGcpAyM7PGcpAyM7PGcpAyM7PGcpAyM7PGcpAyM7PG6nmQkvQNSYsk3dWWtp2kayXNrd/bVroknS1pnqQZkg7oXcnNzKzbeh6kgG8Chw1IOx2YEhETgCn1GeBwYEL9TAbOHaIymplZD/Q8SEXET4HfDkg+Criwpi8Ejm5LvyjSLcAoSTsOTUnNzGyo9TxIrcGYiHiophcCY2p6LPBA23LzK+13SJosaaqkqYsXL+5eSc3MrGuaGqT+W0QEEC/ie+dFxKSImNTX19eFkpmZWbc1NUg93BrGq9+LKn0BMK5tuZ0rzczMhqGmBqmrgBNr+kTgyrb0E+opvwOBJ9qGBc3MbJgZ2esCSPoP4BBgtKT5wN8BnwUuk3QycB9wXC1+DXAEMA9YCpw05AU2M7Mh0/MgFRHvXsOsQwdZNoBTulsiMzNriqYO95mZmTlImZlZczlImZlZYzlImZlZYzlImZlZYzlImZlZYzlImZlZYzlImZlZYzlImZlZYzlImZlZYzlImZlZYzlImZlZYzlImZlZYzlImZlZYzlImZlZYzlImZlZYzlImZlZYzlImZlZYzlImZlZYzlImZlZYzlImZlZYzlImZlZYzlImZlZY62XQUrSYZLukTRP0um9Lo+ZmXXHehekJI0AvgocDuwNvFvS3r0tlZmZdcN6F6SAVwLzIuLXEfEM8C3gqB6XyczMukAR0esyrBNJxwKHRcQH6vN7gVdFxF8MWG4yMLk+7gnc8yJWNxp45PcortfXu/UN523z+ry+oVrfrhHR1+nCrIuRvVx5N0XEecB5v08ekqZGxKQOFcnrG8L1Dedt8/q8vqavr5PWx+G+BcC4ts87V5qZmQ0z62OQ+gUwQdJukjYB3gVc1eMymZlZF6x3w30RsVLSXwA/BEYA34iIWV1a3e81XOj19XR9w3nbvD6vr+nr65j17sEJMzPbcKyPw31mZraBcJAyM7PGet4gJenTkmZJmiFpuqRXdWLFkp6q3+Ml3bWGZfaqdd4haffWd5pC0vsknSNpVZWz9TNe0jclLZO0XNJSSZdIGiXpUUmj15DfU4Plv4ZlB603STtI+pakX0maJmm+pI8OstwkSWcP2I6nJJ0h6cwBeVwj6eUDvn9DvZrqTkk3SdpzXeruxVhbfQxY7nBJUyXNrrbzhUr/Zv2dHZJOlfTd2sYZkh6SNFnS1WvI89417bcBy50h6RODpH9G0hOSVlR7uEnSy1v7UNIhA9e9tmNjkPxPk7TFupZ3XUn6uqSQ9ECrP5A0sdJ+Z7u7sP4bJK31Ueoqy7+1ff6JpMfa67fq9s/aPq+q9rxY0u2SXt223PPug8H6phdS1k6QtJGksyXdJWmmpF9I2q3mXSNpVHsZB2tr67CuQdv3i8zrfZJ2er7l1hqkJB0EvAU4ICL2Bf4UeKATBXyBjgYuj4j9I+JXQ7VS5auX1sWyiJjY+gEOJp86fGVEbAaMB2YCo4Ct2tbT0QdXJAm4ArghInaPiFcA04BtBll8ekR8bA1ZnTggj08BYwZZ7j0RsR9wIfD5dShn1x7YkfRHwDnA8RGxNzAJmDdgGQF/D8yubdwXOIzBt3Fd1j3odtVx9BfApyNiU2AX4O9+3/UNcBqwxfMuleWRpHUeRantOAiYBcyhvz94N3Dnuub3Ate5rsciwNPAH0navD5vCywasMx44M/aPi+LiD3rD1c/BXzmRay3a56nHt4J7ATsGxH7AMcAjwNExBER8fgQFPHFeB9Z7rWLiDX+AG8DvreGefeSO3I6MBU4gHzi7lfAn9cyLwGmALeTnfRRbd9/qn6PB+4aJP8jgIXk30BdP+A7IjvFuyrfd1b6V4G31vQV5JN/AO8H/qmmjwduq3L/CzCilTfwBfJge81aljsJ+GXN+1eyQ3xqQNl/CiwZuN3kK5wCWAo8AywG3gHcXOtdDcwA7gDOqnp9pMo2v34vIN+ecQ+wDHiMbJC/rfnPAHcDvyED1MO13qnAiirX0lrnFLLDWV75rK7yPgI8Wfn/Evhj4JvAucCDtY5l5J8DzAFuJDuGKyuPRWTn9Vj97A48Czxav5+pfTuv0pYD/1br/t9kG4oqw+eAi4A31HJ3t5XtnLY6f2Nt06NVrpdU+t/W57uqzs6uskXV5XTyjSR3AYcAN9RyK6rsH618Hqv9Mgu4H/gg8ElgLvDlque5wBnA/wMuASZW/ayoevkueUzsAdxadbYaeII8npa3Lb+q6rGVdjvw66qjy9u2U8DHavmHgOurXCuqrGeSbXl65fXLSt+t6n95/Xy3tvNjwGyyHX5rkP7g58DVZBsfU+u/k2yDf1vL/WPtozuB75HtbA7Z9u4j336wdW3/nbUvJwP71X55mjwWVwCHAn3Ad2p7nwQuq+2ZRf75yW2Vz49relXto/lkP7CUbG9PAh8iX622pJZ7Gvhn8tg5pLbtHbX89KqLVeSJ28sqz6erzJ9rq5sVtW/uBD5baVMrbUZt9/+t9InALZV+VZVxdn13KdmOvlVlOos8tmfX/p1LttXz6H/w7Ypa5r/3GfAnVf5nKn0r+vvPQ4Cra/p/kO365FYbqPQ3AFcM0i+fAVxMHmtzgQ+2zftk7aMZwJlt/fscsq+cBfwI2Bw4trbvnirn5muMQ88TpF5SGfwS+BrwJwOC1Idr+kttFdEHPFzpI4Gta3o0eVC0KnatQaqtQj4xSGB7O3At+Qj6GLJB7khevXy+lrkNuKWmLwDeBPwBedBsXOlfA06o6QCOq+lBl6t13F/buAlwExmkVlU9Ta8G06qzXwLfIBu2altXkY1sBPAR8ipkJHmgrCYP9plkZ76S7DQfIg+Md9UOv4vsyFdWY7mfbOytzvs68uoX8qB7tBrFwzV/fE0vqvW2gtZTZIf1W+DTwKnkQbIVGaR+VvV+TK37EfJMaHGVd2Jtw0PklcrMqoMvV/0uBi4F/g/9B/s+lf80suO7rr5/R23XNDIwbll5LCSvDL9QdTWObFs/rWVur/W1Oszt2trPr2obTq18Hmhvg+TBuwz4PrBx1cnhtcz9tdyPyf1+JHBg1eHXyEB9W233D8nOcBbZ8fwN/R3bbbUP7yY7y2fIQNqqq3m1bxaSHdGs2qbdyHdUriL/gP0AMri9psr3DLAvGazPI4/PPrL93EQGxtVkZ38C8B7g0ba62bV+PwhsWtOjBukP5pHt5Maq54PJ46s9SB1KXolDdlpX1fSPah+OJoPSVyt9dv38VS0f5LFxc83/97btvBl4rKYvbtuHXyDbzZb0n4xdQQbMqP1xDRk49qkyXk1eDX6n6rW1bU8Ar6h830IeE9uQV8NnVPretU93I192vQrYor3NVV4fBv6DbNdfrvQZVF9KHk8zanoReVxNIkddAvhPclQAsv39srbxYuDISl9Y+3s62R/tT/ZfB1f6rmQf85wgBbyaPL52Ifunu4G+tjo/cg198p1koBlNnvDtRH+7EzlCdzXw2irzSmBiff+ytu25AZi0thgUEWsf7ouIp4BXkA1qMXCppPe1LdL6I9qZwK0RsSQiFgMrahxUwD9LmkEe3GPpzBDHa4D/iIhVEfEw8BPyjOBG4I+Vb0WfDTwsaUdyiOLnZMN8BfALSdPr88sqz1VkY2Uty72KPPgWR77c9tJavn247xiyM7idDERH1Ha3D639a0SsIhvIy4BvA/+z6muzKsuTlcdKsiGPBP4B2IEMouNquT3Ixn0z2ahV9XNBlf3V5EF7MHnmcz95ANxJDoNMIc/QZ1TZlpJB7SSyI9w3IpbUvCXkAXcn2SluTHb6q8mDZ9cq01yyo76RDObja5m7yTYzrdYxAngd2QHMIoerXkWeZW8PvJT+E5unqy6+FxFPkJ39klrngWSncROwFxlAdq0yv07SrZJmVt3Nr3p6FtiO3/Uk2Zk8Sx7021f6ZlXGl9f2/GFtx1Zkh7Ci9sFOZMd1BxkkxpId7nLy4N208ti16nIEeeY+tr73slrX9mTn9zJyH08hO2KAlRFxe9XpREn71fSDZGfxxirHdVXOP6x6fxb4o8rzJmDruqf46aoXyHZwiaTjq77/W/UHk6njvbbrzNqOdrtXuWaSgbPVfs6sMkC2L0m6k+wTdq1y/zO5f5aQ7QcykJxT7XkfYKWkl5DHwZhaz4erzqaSbW4pMIE8OV1FnnQdXMvsS3a2ryVPsP+QPIH4ANlxHgZcVPf0vkQGwieqHB+XtIzc95vXOv4UeDYillY9/VbSNuQx++fkCdAHgddW+qiI+EmrWsn9DXm8LyH3+8oq9y7A6bXt11c93QG8vsoN2U5vJ08CVpJtZSHwRbJ9bh0Rz9mXVXfnkYHo/siocTFwfPXdBwH/xeCujIhlEfFIlemV9Le7O6ose1XdAPwmIqbX9DTy+HnBnndcugLBDRHxd+SZxNvbZq+o36vbplufR5Jna33kWclE8gxxM9ZA0gV1M/aaddmItrIuIDvgw8gz6xuB48gziCVkB35hW0DZMyLOqK8vr8DB8yz3QiwjO4K9ImIHMsAf2Tb/yfq9ityR15NBdRmwsOrqL9uWawWfU4AfkI3y5WQw+Co5NPAs/ScNq8iz5ImV9um2da+q/Nq1N+BnyIPvtWTwO0DSCW3fhdy/QQao08iGuawtj1Z72LiWG1lpK+kfyppLnoVtTnaYW5FDak/Weg8hg/Nq+l97tYr+dtYqy8iqm2tre79DDu2eLGkz8irn2Mix+rlkm59V6xO/a/WAuhpZ+Wxfef+EHLrYrALZcrLD+DnZ3nYjzzDvaZWLHCr6Va33NfRf8X6EbHf7kB3/XHI/3kWewb+ZDOb/RQ5JfqC2v3UMzSUD9DvJs3pqnZ8hA9bryCu7c8mTpbmtthwR95JXZD8ij+s59f03k23qAPIkbeB9ttVk2/oEedJxINkprqa/Pzmj1rUPecxvChARN9UyB5PH6b5kZ3ga/VflV5Jt5mD6g9RGwIG1f6eSIx5P1bY+UnU6B9gnIv6g9tu7ybP2fcn280/AjRGxGzmUdQfZRxzJgD4pIm4m9+F3yOHhVpt7edXXlmT7WR0RP2Ltfk7uh03q81uAHaqfG/hQxZvJvmIv8opyeW3j28l63gLYIyJeTrXBtu99hTwW3gR8tr77gfr+NZL2GrCuh2qZ/dvSLiCHht8NfDvyxQmnqP+BsNb9o4H9R6t/+kxbn7lHRJxf89tjwyrW8SUSz/fgxJ6SJrQlTSTHVl+obYBFEfGspNfRf3Y7qIg4qTbwiOfJ90bgnZJGSOojO9Tbat4tZKNvBalP0N/YpwDHSnopgKTtJA1WpjUtdyvwJ5K2l7QxeQY8mNlkQ9uttruPbNitQNluBHkQ/ZA6mMsubdNvIA/GY8mDbix5dQcZCEaTB8FCsrN4hLyyhLxaOo0MKHuQZ62PVj6PkR0sZCcI2XA3IQ/e75OB5I3kVc0cskNstZsD6K932qa3ru16Kzl0OJgtgRUR8TnygNyG3HejyM7qSbLjPqbqZm1uAQ6WtAd5r/LTkt5I/0F8dJ15t/b1dfQHz5ZNyXtvi4EPtXXOW7bl8zfkEE7705JPAO+lv71NAhbUmffDZP1uRO7Px8kh46VkcN4D2FjSIeTV7PZkMJtAdqrvJ+vxGXKfTKzvtcwjrzyPJQP7VlVX76e/nc0k99n20N+WlU9rjoyIT5JXIePqYYpxEXE98NfkPnlJa2XKJzjHtq3/duAndXL3eNu8bYFn6hhpDXNT+2QEeY/3BnLYbikZUHepemudAB1BtlnIwNBe5x+p37PI/fZkbfeZ9VDMqlr+G+S+Hlf7gNru7ch2vRV58/45qkPfqrbve22zHiSPkxHkPh8haUvyRGRj1dOVkrar/b+SvHK4hrwf+dOIuIQ8jj4aEVMr32VV968jT3LOpv9hpx/WtrTa4Nhqy62nVDciT8rvoX+f7U+emM+surmD7I/aPU4Gt89U+yMiHqxt/F9kwCIivtoWeB6s7x4laTNJ25Mnk7+ocr6/yoaksa3+cy2W0PYg2RqtbSyQHPL6Of03Uv8TGB3996Ra0+/juTex7yU7ztHkEMjM2ug5wPjWeG3bOOu63pMa9MGJmncy8GBNb0yeYb6tbf47ySuRGWQDOrA97xewXPuDE+cx+IMTh5Id0WrygFkFfLPmPUsOrXye7NBur/ym13LPkAffdHKI9MdkA3+U/hvA95KBaDnZEX6DHEpZUGlPkWcvy2t9M+h/cGJx1dmZ9D840XoI4qnK+way832GPNCuI88qj61y31Pb9qnaph8AU6P//uTqWve0mnd1leP6yuNY+m/azyCHiy6q9F/UPmvd0A9g+8p7OdXOatkHgEPq8+vpv2n7m8p/Tm3vY+TV2lzgslr+A5X3ikpfQg7JfJ8cJplddXxxLf945dvK56ZKn151tGV9fpT+ezAT6b9Cbg3fPEv/fY/WgxPLyCCzrK1eWvcnl9eys8grqiX0H0PnVB1cT3Zk99T0qbXvZpPHX+thiNY+OZA8wVpa61xGPrDSGr6dSR5bpw/SH9xVZRjYH3yj6noq2Wm37r99nez47iLP/lv3RV9a2zOHbLd2MfMAAAJ0SURBVN/L6X+wYAV1nyb672dfWut8mv6Hi2ZVGWbW9t1X062r7zk1f0H9PFn186fkMbekyvNb+u9JPUEOZ0eVubX/3kpeSbX20+Lap9tUGVfT/1DQk8DHee6DE3eTD7xsxHMfnPgeefXYam+th5VOJ9vJ5mRQn1nrfJpsgxeQ/ePGVc5lVYcLa198rdKfIY/dTRn8wYldqh5fVZ/fRd3LX0uffBGDPzhxapVzZs3fnQH9O3nR0Lqv93ZewIMTfi1Sw9U9wEkx4P9lma0PJG0KrKqho4PIG/I/i4j39rhojVCPlm8cEcsl7U6elO4Zec+7F+U5B7ijbaiu59a7F8ya2XplF+CyGpbagTyz/4feFqlRtgCubxsa/UgPA9Q08krtL3ux/jXxlZSZmTWW391nZmaN5SBlZmaN5SBlZmaN5SBl1kFahzf1v5g3Sq9L/mbDgYOUmZk1loOUWZdJOrLeH3iHpB9Lan9/5X6SbpY0V9IH277zSeX/BZoh6cweFNusERykzLrvZ+QbS/Yn37P4V23z9iXflnEQ8LeSdqrXB00gX9w5EXiFpNcOcZnNGsF/zGvWfTuT/0FgR/IdfL9pm3dlRCwj39/WeqP0a+h/ozTk+/MmkO8HNNugOEiZdd9XgC9GxFX1Ms8z2uat7Y3S/zI0xTNrLg/3mXXfNvT/u5ETB8zr1BulzYYlX0mZddYWkua3ff4ieeX0bUmPkW+U361t/gzyzdyjgX9o/bsESX8A3Jz/eYKnyP/zs6j7xTdrFr+7z8zMGsvDfWZm1lgOUmZm1lgOUmZm1lgOUmZm1lgOUmZm1lgOUmZm1lgOUmZm1lj/H2hM/U/Q63rNAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHs6qP4jo_T_"
      },
      "source": [
        "dataset = np.load('/images.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiwUdu-93fWe"
      },
      "source": [
        "dataset.reshape((4750,128,-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIh31KI0qBgN"
      },
      "source": [
        "dataset.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A66qIVSnz2qg"
      },
      "source": [
        "X = dataset # Convert the features (pixel values) to numpy array to feed into the supervised learning model.\n",
        "y = y.values # Convert the labels to numpy array to feed into the supervised learning model."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEoI4Wjs1ukl"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3j662UX0M84"
      },
      "source": [
        "# Split data into test and train to build the model.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABkHdEG-0RBx"
      },
      "source": [
        "type(X_train) # As we can see that the data to be fed into model is of the type numpy array."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umObWk520Yti"
      },
      "source": [
        "i = 0\n",
        "image = X_train[i]\n",
        "label = y_train[i][0]\n",
        "plt.imshow(image);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XQHYSh94Eq_"
      },
      "source": [
        "sobel = cv2.Sobel(image, cv2.CV_64F, 1, 1, ksize=5)\n",
        "plt.imshow(sobel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEJp8uTN4eq8"
      },
      "source": [
        "# Step 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqD9jDH35OJa"
      },
      "source": [
        "# Normalize the data\n",
        "X = X.astype('float32') / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5BuS4EL6efF"
      },
      "source": [
        "print(len(X)) # Check the number of rows in the dataset.\n",
        "print(X.shape) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q90BHoHz8jeH"
      },
      "source": [
        "new_x = np.empty((4750, 128, 128)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcu6nI7y9FW5"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCt_bRu38_v8"
      },
      "source": [
        "# Gaussian blurring\n",
        "X_new = np.apply_along_axis(\n",
        "    lambda img: cv2.GaussianBlur(img, (5, 5), 0), \n",
        "    -1, \n",
        "    X.reshape((-1, 128, 128))\n",
        ")\n",
        "X_new = X_new.reshape((-1, 128, 128))\n",
        "plt.imshow(X_new[3], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjjBdAiuADfC"
      },
      "source": [
        "# Visualize the data\n",
        "df = pd.DataFrame({'A' : np.random.randn(8),\n",
        "                   'B' : np.random.randn(8),\n",
        "                   'C' : np.random.randn(8)})\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtYWe3byAHVQ"
      },
      "source": [
        "df.apply(lambda row: row.mean(), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VztNWziAAMEA"
      },
      "source": [
        "# Now we create another empty numpy array of 4750 rows and 16384 columns shape, to store the pixel values, which are obtained by applying the Gaussian Blur.\n",
        "# The new array is necessary as we have to feed the data in the model in this original format.\n",
        "another_x = np.empty((4750, 16384))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tv79V2AOAXd-"
      },
      "source": [
        "# Append/Set the values of the another array to be fed into model equal to the flattened array, which has the shape of 784 pixels (1-D)\n",
        "for idx, img in enumerate(new_x):\n",
        "  another_x[idx] = img.flatten()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ska0bgWCAc0c"
      },
      "source": [
        "plt.hist(another_x[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoNg1Mhm4nMv"
      },
      "source": [
        "# Step 3 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUdqAeiPp44m"
      },
      "source": [
        "# Convert labels to one hot vectors\n",
        "\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "enc = LabelBinarizer()\n",
        "y_train = enc.fit_transform(y_train)\n",
        "y_test = enc.fit_transform(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytFLn2Vn_6pc"
      },
      "source": [
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPLSkWYR_74W"
      },
      "source": [
        "# Print y_train[0]\n",
        "y_train[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39nD2pNP5vRg"
      },
      "source": [
        "# Split the dataset into training,testing, and validation set.(Hint: First split images andlabels into training and testing set with test_size = 0.3. Then further split test data into test and validation set with test_size = 0.5)\n",
        "X_train = X_train.reshape(X_train.shape[0], 128, 128, 3)\n",
        "X_test = X_test.reshape(X_test.shape[0], 128, 128, 3)\n",
        "\n",
        "# Print the shape of the data. It is in the proper shape.  \n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rnQdKJ_BMxV"
      },
      "source": [
        "# Step 4\n",
        "# 4.Building CNN: (15 Marks) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "simAY1BPBhAB"
      },
      "source": [
        "<h1>Create the Model:<h1/>\n",
        "\n",
        "- Convolutional input layer, 32 feature maps with a size of 5×5 and a rectifier activation function.\n",
        "- Batch Normalization Layer.\n",
        "- Convolutional layer, 32 feature maps with a size of 5×5 and a rectifier activation function.\n",
        "- Batch Normalization layer.\n",
        "- Max Pool layer with size 2×2.\n",
        "- Dropout layer at 25%.\n",
        "---\n",
        "- Convolutional layer, 64 feature maps with a size of 3×3 and a rectifier activation function.\n",
        "- Batch Normalization layer.\n",
        "- Dropout layer at 25%.\n",
        "- Convolutional layer, 64 feature maps with a size of 3×3 and a rectifier activation function.\n",
        "- Batch Normalization layer.\n",
        "- Max Pool layer with size 2×2.\n",
        "- Dropout layer at 25%.\n",
        "---\n",
        "- GlobalMaxPooling2D layer.\n",
        "- Fully connected layer with 256 units and a rectifier activation function.\n",
        "- Dropout layer at 50%.\n",
        "- Fully connected output layer with 10 units and a softmax activation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd4qw2Q35Ndi"
      },
      "source": [
        "# Set the CNN model\n",
        "# Define layers\n",
        "batch_size = None\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (5, 5), padding='same', activation=\"relu\", input_shape=X_train.shape[1:]))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Conv2D(64, (5, 5), padding='same', activation=\"relu\"))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.3))\n",
        "model.add(layers.Conv2D(64, (3, 3), padding='same', activation=\"relu\"))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.4))\n",
        "model.add(layers.Conv2D(64, (3, 3), padding='same', activation=\"relu\"))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.5))\n",
        "\n",
        "model.add(layers.GlobalMaxPooling2D())\n",
        "model.add(layers.Dense(256, activation=\"relu\"))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(12, activation=\"softmax\"))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R76ytslbB5p8"
      },
      "source": [
        "# Number of parameters in the CNN layers:\n",
        "\n",
        "- ## Number of Parameters of a Conv Layer:\n",
        "In a CNN, each layer has two kinds of parameters : weights and biases. The total number of parameters is just the sum of all weights and biases.\n",
        "\n",
        "  Let’s define,\n",
        "\n",
        "  - W_c = Number of weights of the Conv Layer.\n",
        "  - B_c = Number of biases of the Conv Layer.\n",
        "  - P_c = Number of parameters of the Conv Layer.\n",
        "  - K = Size (width) of kernels used in the Conv Layer.\n",
        "  - N = Number of kernels.\n",
        "  - C = Number of channels of the input image.\n",
        "\n",
        "    W<sub>c</sub> = K<sup>2</sup> x C x N\n",
        "    \n",
        "    B<sub>c</sub> = N\n",
        "\n",
        "    P<sub>c</sub> = W<sub>c</sub> + B<sub>c</sub>\n",
        "\n",
        "\n",
        "  So, we will use this formulae for our parameter calculation of first layer:\n",
        "    \n",
        "    model.add(layers.Conv2D(32, (5, 5), padding='same', activation=\"relu\", input_shape=x_train.shape[1:]))\n",
        "\n",
        "  So the number of parameters is given by:\n",
        "\n",
        "  Number of weights = W<sub>c</sub> = 5<sup>2</sup> x 3 x 32 = 2400\n",
        "\n",
        "  Number of biases = B<sub>c</sub> = 32\n",
        "\n",
        "  Total number of parameters for this layer = P<sub>c</sub> = 2400 + 32 = 2432\n",
        "\n",
        "\n",
        "  We can verify this number by looking at the model summary.\n",
        "\n",
        "    conv2d (Conv2D)              (None, 32, 32, 32)        2432\n",
        "\n",
        "- ## Number of Parameters of a MaxPool Layer:\n",
        "There are no parameters associated with a MaxPool layer. The pool size, stride, and padding are hyperparameters.\n",
        "\n",
        "- ## Number of Parameters of a Fully Connected (FC) Layer:\n",
        "There are two kinds of fully connected layers in a CNN. The first FC layer is connected to the last Conv Layer, while later FC layers are connected to other FC layers. Let’s consider each case separately.\n",
        "\n",
        "  - ### Case 1: Number of Parameters of a Fully Connected (FC) Layer connected to a Conv Layer:\n",
        "  Let’s define,\n",
        "\n",
        "    W_{cf} = Number of weights of a FC Layer which is connected to a Conv Layer.\n",
        "\n",
        "    B_{cf} = Number of biases of a FC Layer which is connected to a Conv Layer.\n",
        "    \n",
        "    O = Size (width) of the output image of the previous Conv Layer.\n",
        "    \n",
        "    N = Number of kernels in the previous Conv Layer.\n",
        "    \n",
        "    F = Number of neurons in the FC Layer.\n",
        "\n",
        "      - W<sub>cf</sub> = O<sup>2</sup> x N x F\n",
        "    \n",
        "      - B<sub>cf</sub> = F\n",
        "\n",
        "      - P<sub>cf</sub> = W<sub>cf</sub> + B<sub>cf</sub>\n",
        "\n",
        "    Example: The first fully connected layer of our model is connected to a Conv Layer. For this layer, O = 1 (the width and height of image after global maxpooling), N = 64 and F = 256. Therefore,\n",
        "\n",
        "      - Number of weights = W<sub>cf</sub> = 1<sup>2</sup> x 64 x 256 = 16384\n",
        "\n",
        "      - Number of biases = B<sub>c</sub> = 256\n",
        "\n",
        "      - Total number of parameters for this layer = P<sub>c</sub> = 16384 + 256 = 16640\n",
        "\n",
        "    We can verify this number by looking at the model summary.\n",
        "      \n",
        "            dense (Dense)                (None, 256)               16640 \n",
        "\n",
        "  - ### Case 2: Number of Parameters of a Fully Connected (FC) Layer connected to a FC Layer\n",
        "  Let’s define,\n",
        "\n",
        "    W<sub>ff</sub> = Number of weights of a FC Layer which is connected to an FC Layer.\n",
        "\n",
        "    B<sub>ff</sub> = Number of biases of a FC Layer which is connected to an FC Layer.\n",
        "    \n",
        "    P<sub>ff</sub> = Number of parameters of a FC Layer which is connected to an FC Layer.\n",
        "    \n",
        "    F = Number of neurons in the FC Layer.\n",
        "    \n",
        "    F<sub>-1</sub> = Number of neurons in the previous FC Layer.\n",
        "\n",
        "      - W<sub>ff</sub> = F<sub>-1</sub> x F\n",
        "\n",
        "      - B<sub>ff</sub> = F\n",
        "\n",
        "      - P<sub>ff</sub> = W<sub>ff</sub> + B<sub>ff</sub>\n",
        "\n",
        "    In the above equation, **F<sub>-1</sub> x F** is the total number of connection weights from neurons of the previous FC Layer the neurons of the current FC Layer. The total number of biases is the same as the number of neurons (F).\n",
        "\n",
        "    Example: The last fully connected layer of AlexNet is connected to an FC Layer. For this layer, **F<sub>-1</sub>** = 256 and **F** = 10. Therefore,\n",
        "\n",
        "      - Number of weights = W<sub>cf</sub> = 256 x 10 = 2560\n",
        "\n",
        "      - Number of biases = B<sub>c</sub> = 10\n",
        "\n",
        "      - Total number of parameters for this layer = P<sub>c</sub> = 2560 + 10 = 2570\n",
        "\n",
        "    We can verify this number by looking at the model summary.\n",
        "      \n",
        "            dense_1 (Dense)              (None, 10)                2570"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O896U4lXBUFh"
      },
      "source": [
        "# initiate Adam optimizer\n",
        "opt = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zr6qxvhHBU10"
      },
      "source": [
        "# Let's train the model using RMSprop\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3qPCglxCEMr"
      },
      "source": [
        "# Network structure is summarized which confirms our design was implemented correctly.\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MP81fOFdCV6Y"
      },
      "source": [
        "X_train = X_train.astype('float32') # Conversion to float type from integer type.\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255.0 # Division by 255\n",
        "X_test /= 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkLTigpfCb3e"
      },
      "source": [
        "#Adding Early stopping callback to the fit function is going to stop the training,\n",
        "#if the val_loss is not going to change even '0.001' for more than 10 continous epochs\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10)\n",
        "\n",
        "#Adding Model Checkpoint callback to the fit function is going to save the weights whenever val_loss achieves a new low value. \n",
        "#Hence saving the best weights occurred during training\n",
        "\n",
        "model_checkpoint =  ModelCheckpoint('cifar_cnn_checkpoint_{epoch:02d}_loss{val_loss:.4f}.h5',\n",
        "                                                           monitor='val_loss',\n",
        "                                                           verbose=1,\n",
        "                                                           save_best_only=True,\n",
        "                                                           save_weights_only=True,\n",
        "                                                           mode='auto',\n",
        "                                                           period=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEp_CBtQCfPq"
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 128, 128, 3)\n",
        "X_test = X_test.reshape(X_test.shape[0], 128, 128, 3)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIhb1CsZCOjy"
      },
      "source": [
        "#Step 5.Fit and evaluate model and print confusion matrix. (10 Marks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9-bIsflQuSV"
      },
      "source": [
        "# Set the batch size, number of epochs.\n",
        "batch_size = 30\n",
        "num_classes = 12\n",
        "epochs = 40\n",
        "num_predictions = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzAClUba5rW1"
      },
      "source": [
        "# Fit the model \n",
        "history = model.fit(X_train,\n",
        "                    y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    shuffle=True,\n",
        "                    verbose=1,\n",
        "                    callbacks=[early_stopping,model_checkpoint])\n",
        "\n",
        "# plot training history\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0apqt1Z95nNJ"
      },
      "source": [
        "# Score trained model\n",
        "scores = model.evaluate(X_train, y_train, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jk3CeSv4C_rY"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUturlDjS4JT"
      },
      "source": [
        "# Confusion Matrix\n",
        "print(\"=== Confusion Matrix ===\")\n",
        "cm = confusion_matrix(y_test, y_pred=)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkAnvZkhS6P-"
      },
      "source": [
        "df_cm = pd.DataFrame(cm, index = [i for i in \"0123456789\"],\n",
        "                     columns = [i for i in \"0123456789\"])\n",
        "plt.figure(figsize = (10,7))\n",
        "sns.heatmap(df_cm, annot=True, fmt='d')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj72xJHHS8Xw"
      },
      "source": [
        "print(\"=== Classification Report ===\")\n",
        "print(classification_report(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBRuQpxhC4MX"
      },
      "source": [
        "#Step 6.Visualize predictions for x_test[2], x_test[3], x_test[33], x_test[36], x_test[59]. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdnABTfrQyup"
      },
      "source": [
        "i = 0\n",
        "image = X_test[2]\n",
        "label = y_test[i][0]\n",
        "plt.imshow(image);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yosjAjw1RtUg"
      },
      "source": [
        "i = 0\n",
        "image = X_test[3]\n",
        "label = y_test[i][0]\n",
        "plt.imshow(image);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m6Uo19tRwq3"
      },
      "source": [
        "i = 0\n",
        "image = X_test[33]\n",
        "label = y_test[i][0]\n",
        "plt.imshow(image);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyoYFN3ERylX"
      },
      "source": [
        "i = 0\n",
        "image = X_test[36]\n",
        "label = y_test[i][0]\n",
        "plt.imshow(image);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6AOTI3XR0QH"
      },
      "source": [
        "i = 0\n",
        "image = X_test[59]\n",
        "label = y_test[i][0]\n",
        "plt.imshow(image);"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}