{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twttier Airline Sentiment Analysis Project 8.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQwVtk_sE4CD",
        "outputId": "bdcc2e16-6b3c-4430-81a8-12a7605f5305"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNjiXgQMH7-w"
      },
      "source": [
        "1. Import the libraries, load dataset, print shape of data, data description. (5 Marks)\n",
        "2. Understand of data-columns: (5 Marks)\n",
        "a. Drop all other columns except “text” and “airline_sentiment”.\n",
        "b. Check the shape of data.\n",
        "c. Print first 5 rows of data."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-rCYpDmG0d2"
      },
      "source": [
        "SEED = 42"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLy4l1u4G25-"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_colwidth', None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PTYCY2mHLan"
      },
      "source": [
        "FILE_NAME='Tweets.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z45WGXmMHR0o"
      },
      "source": [
        "data = pd.read_csv(FILE_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7SqOy1IHVv0",
        "outputId": "4ec77775-07f6-4933-adc1-e5ae1486c75c"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14640, 15)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "id": "rbzdSRPcHWk1",
        "outputId": "6d49ef62-e595-4d14-edb1-8a0780418723"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "      <th>negativereason</th>\n",
              "      <th>negativereason_confidence</th>\n",
              "      <th>airline</th>\n",
              "      <th>airline_sentiment_gold</th>\n",
              "      <th>name</th>\n",
              "      <th>negativereason_gold</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_coord</th>\n",
              "      <th>tweet_created</th>\n",
              "      <th>tweet_location</th>\n",
              "      <th>user_timezone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>570306133677760513</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>cairdin</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:35:52 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eastern Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>570301130888122368</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.3486</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:59 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>570301083672813571</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.6837</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yvonnalynn</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica I didn't today... Must mean I need to take another trip!</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:48 -0800</td>\n",
              "      <td>Lets Play</td>\n",
              "      <td>Central Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>570301031407624196</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Bad Flight</td>\n",
              "      <td>0.7033</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;amp; they have little recourse</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:36 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>570300817074462722</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Can't Tell</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:14:45 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
              "0  570306133677760513           neutral                        1.0000   \n",
              "1  570301130888122368          positive                        0.3486   \n",
              "2  570301083672813571           neutral                        0.6837   \n",
              "3  570301031407624196          negative                        1.0000   \n",
              "4  570300817074462722          negative                        1.0000   \n",
              "\n",
              "  negativereason  negativereason_confidence         airline  \\\n",
              "0            NaN                        NaN  Virgin America   \n",
              "1            NaN                     0.0000  Virgin America   \n",
              "2            NaN                        NaN  Virgin America   \n",
              "3     Bad Flight                     0.7033  Virgin America   \n",
              "4     Can't Tell                     1.0000  Virgin America   \n",
              "\n",
              "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
              "0                    NaN     cairdin                 NaN              0   \n",
              "1                    NaN    jnardino                 NaN              0   \n",
              "2                    NaN  yvonnalynn                 NaN              0   \n",
              "3                    NaN    jnardino                 NaN              0   \n",
              "4                    NaN    jnardino                 NaN              0   \n",
              "\n",
              "                                                                                                                             text  \\\n",
              "0                                                                                             @VirginAmerica What @dhepburn said.   \n",
              "1                                                        @VirginAmerica plus you've added commercials to the experience... tacky.   \n",
              "2                                                         @VirginAmerica I didn't today... Must mean I need to take another trip!   \n",
              "3  @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse   \n",
              "4                                                                         @VirginAmerica and it's a really big bad thing about it   \n",
              "\n",
              "  tweet_coord              tweet_created tweet_location  \\\n",
              "0         NaN  2015-02-24 11:35:52 -0800            NaN   \n",
              "1         NaN  2015-02-24 11:15:59 -0800            NaN   \n",
              "2         NaN  2015-02-24 11:15:48 -0800      Lets Play   \n",
              "3         NaN  2015-02-24 11:15:36 -0800            NaN   \n",
              "4         NaN  2015-02-24 11:14:45 -0800            NaN   \n",
              "\n",
              "                user_timezone  \n",
              "0  Eastern Time (US & Canada)  \n",
              "1  Pacific Time (US & Canada)  \n",
              "2  Central Time (US & Canada)  \n",
              "3  Pacific Time (US & Canada)  \n",
              "4  Pacific Time (US & Canada)  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEIZ6DLCHhAo",
        "outputId": "01250e65-bc9e-44c9-e05d-75fda40b919e"
      },
      "source": [
        "data['airline_sentiment'].value_counts(normalize=True).round(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "negative    0.63\n",
              "neutral     0.21\n",
              "positive    0.16\n",
              "Name: airline_sentiment, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_nkOL5JHuKd",
        "outputId": "819c64ab-a086-4d84-ccce-cf4de8057624"
      },
      "source": [
        "data['airline'].value_counts(normalize=True).round(4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "United            0.2611\n",
              "US Airways        0.1990\n",
              "American          0.1885\n",
              "Southwest         0.1653\n",
              "Delta             0.1518\n",
              "Virgin America    0.0344\n",
              "Name: airline, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgR0fD8nHx8q",
        "outputId": "55c9a941-3f81-4552-ee3f-59526537e37e"
      },
      "source": [
        "data['retweet_count'].value_counts(normalize=True).round(4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     0.9476\n",
              "1     0.0437\n",
              "2     0.0045\n",
              "3     0.0015\n",
              "4     0.0012\n",
              "5     0.0003\n",
              "7     0.0002\n",
              "6     0.0002\n",
              "22    0.0001\n",
              "8     0.0001\n",
              "32    0.0001\n",
              "9     0.0001\n",
              "31    0.0001\n",
              "18    0.0001\n",
              "15    0.0001\n",
              "28    0.0001\n",
              "44    0.0001\n",
              "11    0.0001\n",
              "Name: retweet_count, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qom6vLxDFDsR"
      },
      "source": [
        "3. Text pre-processing: Data preparation. (20 Marks)\n",
        "a. Html tag removal.\n",
        "b. Tokenization.\n",
        "c. Remove the numbers.\n",
        "d. Removal of Special Characters and Punctuations.\n",
        "e. Conversion to lowercase.\n",
        "f. Lemmatize or stemming.\n",
        "g. Join the words in the list to convert back to text string in the dataframe. (So that each row\n",
        "contains the data in text format.)\n",
        "h. Print first 5 rows of data after pre-processing.\n",
        "4. Vectorization: (10 Marks)\n",
        "a. Use CountVectorizer.\n",
        "b. Use TfidfVectorizer.\n",
        "5. Fit and evaluate model using both type of vectorization. (6+6 Marks)\n",
        "6. Summarize your understanding of the application of Various Pre-processing and Vectorization and\n",
        "performance of your model on this dataset. (8 Marks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtWNG07OJ0r0"
      },
      "source": [
        "Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noodQcs2KWo-",
        "outputId": "49bbe71d-af88-4c05-8fbc-81a8a608e313"
      },
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67i4Ug0HFMX2"
      },
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "sid = SentimentIntensityAnalyzer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJFeKPzSJ4ZP"
      },
      "source": [
        "message_text= data.loc[0, 'text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUsq9TAWKlIo",
        "outputId": "8a94f055-3577-4926-b1ef-562c0c05a803"
      },
      "source": [
        "sid.polarity_scores(message_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'compound': 0.0, 'neg': 0.0, 'neu': 1.0, 'pos': 0.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DncI-clKKqZW"
      },
      "source": [
        "scores = data['text'].apply(lambda x: sid.polarity_scores(x)).apply(pd.Series)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cbvRwe4LnXG"
      },
      "source": [
        "scores=scores.apply(pd.Series)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jT5kRNzLSSS"
      },
      "source": [
        "scores.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMz56eqiLTWO"
      },
      "source": [
        "scores['sentiment']=data['airline_sentiment']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2qFxBP5LbiA"
      },
      "source": [
        "scores.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "fd2Z2veGNGi6",
        "outputId": "7440b2a8-b6c8-45ec-e872-ce765902c520"
      },
      "source": [
        "def find_threshold(score,\n",
        "                   actual_distribution={'negative':0.63,\n",
        "                                        'nuetral':0.21,\n",
        "                                        'positive':0.16},\n",
        "                   ):\n",
        "    total= len(score)\n",
        "\n",
        "    for threshold in [x/100 for x in range(-100,100,1]):\n",
        "      total_negatives = sum(score < threshold)\n",
        "      if abs(total_negatives/total - actual_distribution['negative']) <= 0.01:\n",
        "        negative_threshold = threshold\n",
        "        break\n",
        "\n",
        "    for threshold in [x/100 for x in range(int(negative_threshold*100,100,1)]:\n",
        "      total_neutral = len(np.where((score >= negative_threshold) & score <=threshold))[0])\n",
        "      if abs(total_neutral/total - actual_distribution['neutral']) <= 0.01:\n",
        "        nuetral_threshold = threshold\n",
        "        break\n",
        "\n",
        "    return nuetral_threshold, negative_threshold                                 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-626f8f892a8a>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    for threshold in [x/100 for x in range(-100,100,1]):\u001b[0m\n\u001b[0m                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "_Bt2Nop8Ptlo",
        "outputId": "1c1c0759-9493-476d-cb4a-c27dbd52d72f"
      },
      "source": [
        "find_threshold(scores['compound'].values)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-4385927710de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfind_threshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'compound'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'find_threshold' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qwhb6-hztXg"
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "find_threshold(scores['compound'].values)\n",
        "end = time.time()\n",
        "print(\"{:.2f} seconds\"}format(end-start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e16UO8sM1TWi"
      },
      "source": [
        "scores['p_sentiment'] = pd.cut(scores['compound']),\n",
        "                                bins=[-1,0.2023,0.5766,1]\n",
        "                                labels=['negative','nuetral','positive']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Rzb3yYy2YMr"
      },
      "source": [
        "scores.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz-6HhDY2aD5"
      },
      "source": [
        "scores['flag'] = scores['p_sentiment']==scores['sentiment']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VheAQgsg2mic"
      },
      "source": [
        "scores['flag'].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2hcUNMH3Zxt"
      },
      "source": [
        "scores.groupby(by='sentiment')['flag'].agg(['mean','count']).round(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXAMzaxo5U4w"
      },
      "source": [
        "Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhALSmtM3nWh",
        "outputId": "2f1760df-481e-4c6a-b62c-9f18704f1c94"
      },
      "source": [
        "# Import necessary libraries.\n",
        "import re, string, unicodedata\n",
        "import pandas as pd\n",
        "import nltk           \n",
        "                        # Natural language processing tool-kit\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "!pip install contractions\n",
        "import contractions\n",
        "\n",
        "\n",
        "from bs4 import BeautifulSoup                 # Beautiful soup is a parsing library that can use different parsers.\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords, wordnet    # Stopwords, and wordnet corpus\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/5f/91102df95715fdda07f56a7eba2baae983e2ae16a080eb52d79e08ec6259/contractions-0.0.45-py2.py3-none-any.whl\n",
            "Collecting textsearch\n",
            "  Downloading https://files.pythonhosted.org/packages/42/a8/03407021f9555043de5492a2bd7a35c56cc03c2510092b5ec018cae1bbf1/textsearch-0.0.17-py2.py3-none-any.whl\n",
            "Collecting Unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/65/91eab655041e9e92f948cb7302e54962035762ce7b518272ed9d6b269e93/Unidecode-1.1.2-py2.py3-none-any.whl (239kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 4.5MB/s \n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/9f/f0d8e8850e12829eea2e778f1c90e3c53a9a799b7f412082a5d21cd19ae1/pyahocorasick-1.4.0.tar.gz (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 25.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.0-cp36-cp36m-linux_x86_64.whl size=81699 sha256=6b4273fe5c44f7e0eab78064bd70234c401594e4aed544a45c22d293b668b1d7\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/90/61/87a55f5b459792fbb2b7ba6b31721b06ff5cf6bde541b40994\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: Unidecode, pyahocorasick, textsearch, contractions\n",
            "Successfully installed Unidecode-1.1.2 contractions-0.0.45 pyahocorasick-1.4.0 textsearch-0.0.17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7KCHI094vWw"
      },
      "source": [
        "def replace_contractions(text):\n",
        "    \"\"\"Replace contractions in string of text\"\"\"\n",
        "    return contractions.fix(text)\n",
        "\n",
        "# Perform the above operation over all the rows of text column of the dataframe.\n",
        "for i, row in data.iterrows():\n",
        "    text = data.at[i, 'text']\n",
        "    clean_text = replace_contractions(text)\n",
        "    data.at[i,'text'] = clean_text\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7sRcYQD4zHp"
      },
      "source": [
        "# Tokenize the words of whole dataframe.\n",
        "for i, row in data.iterrows():\n",
        "    text = data.at[i, 'text']\n",
        "    words = nltk.word_tokenize(text)\n",
        "    data.at[i,'text'] = words\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJoAMaxE4zv_"
      },
      "source": [
        "# save the stopwords in a list named stopwords.\n",
        "stopwords = stopwords.words('english')\n",
        "stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RASlnzek41-W"
      },
      "source": [
        "def remove_non_ascii(words):\n",
        "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
        "    new_words = []                        # Create empty list to store pre-processed words.\n",
        "    for word in words:\n",
        "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        new_words.append(new_word)        # Append processed words to new list.\n",
        "    return new_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xN8WAkFP45OC"
      },
      "source": [
        "def remove_punctuation(words):\n",
        "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
        "    new_words = []                        # Create empty list to store pre-processed words.\n",
        "    for word in words:\n",
        "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
        "        if new_word != '':\n",
        "            new_words.append(new_word)    # Append processed words to new list.\n",
        "    return new_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynKvzW2j4869"
      },
      "source": [
        "def remove_stopwords(words):\n",
        "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
        "    new_words = []                        # Create empty list to store pre-processed words.\n",
        "    for word in words:\n",
        "        if word not in stopwords:\n",
        "            new_words.append(word)        # Append processed words to new list.\n",
        "    return new_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iPL9uEM5AgX"
      },
      "source": [
        "def stem_words(words):\n",
        "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
        "    stemmer = LancasterStemmer()\n",
        "    stems = []                            # Create empty list to store pre-processed words.\n",
        "    for word in words:\n",
        "        stem = stemmer.stem(word)\n",
        "        stems.append(stem)                # Append processed words to new list.\n",
        "    return stems"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXOlzz9z5BGs"
      },
      "source": [
        "def lemmatize_verbs(words):\n",
        "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = []                           # Create empty list to store pre-processed words.\n",
        "    for word in words:\n",
        "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
        "        lemmas.append(lemma)              # Append processed words to new list.\n",
        "    return lemmas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGk4Pl-C5DMK"
      },
      "source": [
        "def normalize(words):\n",
        "    words = remove_non_ascii(words)\n",
        "    words = to_lowercase(words)\n",
        "    words = remove_punctuation(words)\n",
        "    words = remove_stopwords(words)\n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTQmN6Tu5G2J"
      },
      "source": [
        "# Iterate the normalize funtion over whole data.\n",
        "for i, row in data.iterrows():\n",
        "    words = data.at[i, 'text']\n",
        "    words = normalize(words)\n",
        "    data.at[i,'text'] = words\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODRdvdVd5I0M"
      },
      "source": [
        "def stem_and_lemmatize(words):\n",
        "    stems = stem_words(words)\n",
        "    lemmas = lemmatize_verbs(words)\n",
        "    return stems, lemmas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7tAAgny5K0B"
      },
      "source": [
        "data['lemma'] = ''\n",
        "data['stem'] = ''\n",
        "\n",
        "for i, row in data.iterrows():\n",
        "    words = data.at[i, 'text']\n",
        "    stems, lemmas = stem_and_lemmatize(words)\n",
        "    data.at[i,'stem'] = stems\n",
        "    data.at[i, 'lemma'] = lemmas\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxNTjZRM6-Jc"
      },
      "source": [
        "Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJSV_51w6KJR"
      },
      "source": [
        "ps = PorterStemmer()\n",
        "def preprocess(text):\n",
        "  '''Pre-processing steps as described above.'''\n",
        "  # text = text.lower()\n",
        "  # # text = re.sub(r'[.|,|)|(|\\|/]',r'', text)        #Removing Punctuations\n",
        "  # words = [word for word in text if word not in stopwords]   # removing stopwords\n",
        "  # return words\n",
        "  text = text.lower()                 # Converting to lowercase\n",
        "  text = re.sub(r'[.|,|)|(|\\|/]',r' ', text) # Removing punctuation\n",
        "  word_tokens = word_tokenize(text)  # Tokenization\n",
        "\n",
        "  words = [word for word in word_tokens if word not in stopwords] # Stop word removal\n",
        "  return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNSUi-2Y7GAw"
      },
      "source": [
        "Build count and tfidf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiCNHvp37JNw"
      },
      "source": [
        "X = data['text']\n",
        "y = data.airline_sentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORT2kBv57QwS"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5tcfPgd7k6R"
      },
      "source": [
        "import xgboost as xgb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnamuHAL6sXI"
      },
      "source": [
        "cv = CountVectorizer()  \n",
        "X = cv.fit_transform(sent)\n",
        "print(cv.vocabulary_)\n",
        "print(X.shape)\n",
        "print(type(X))\n",
        "print(X.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQvMCOFG6vqI"
      },
      "source": [
        "myvocabulary = list(set(sent))\n",
        "myvocabulary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3L6kwa56xee"
      },
      "source": [
        "corpus = {1: d1, 2: d2, 3: d3, 4: d4}\n",
        "tfidf = TfidfVectorizer(vocabulary = myvocabulary, ngram_range = (1,2))\n",
        "tfs = tfidf.fit_transform(corpus.values())\n",
        "feature_names = tfidf.get_feature_names()\n",
        "corpus_index = [n for n in corpus]\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(tfs.T.todense(), index=feature_names, columns=corpus_index)\n",
        "print(tfidf.vocabulary_)\n",
        "print(tfidf.idf_)\n",
        "print(tfs.shape)\n",
        "print(df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCYgfjWi6zue"
      },
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer \n",
        "sentiment = SentimentIntensityAnalyzer()\n",
        "sentiment_dict1 = sentiment.polarity_scores(d1)\n",
        "sentiment_dict2 = sentiment.polarity_scores(d2)\n",
        "sentiment_dict3 = sentiment.polarity_scores(d3)\n",
        "sentiment_dict4 = sentiment.polarity_scores(d4)\n",
        "\n",
        "print(sentiment_dict1)\n",
        "print(sentiment_dict2)\n",
        "print(sentiment_dict3)\n",
        "print(sentiment_dict4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiiNGkeU62AP"
      },
      "source": [
        "from textblob import TextBlob\n",
        "print(TextBlob(d1).sentiment)\n",
        "print(TextBlob(d2).sentiment)\n",
        "print(TextBlob(d3).sentiment)\n",
        "print(TextBlob(d4).sentiment)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}